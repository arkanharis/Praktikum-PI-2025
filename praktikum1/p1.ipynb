{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d0b944",
   "metadata": {},
   "source": [
    "# PENILAIAN PERTEMUAN 1: TEXT PREPROCESSING & TOKENIZATION\n",
    "**Nama:** [ISI NAMA ANDA]\n",
    "\n",
    "**NPM:** [ISI NIM ANDA]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078cd18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Catatan:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314e6f6",
   "metadata": {},
   "source": [
    "## Import Library dan Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load data\n",
    "with open('crawled_data.json', 'r', encoding='utf-8') as f:\n",
    "    documents = json.load(f)\n",
    "print(f\"Berhasil memuat {len(documents)} dokumen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7935c",
   "metadata": {},
   "source": [
    "## 1. Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40a061",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "sample_text = documents[0]['content']\n",
    "print(\"Original text:\")\n",
    "print(sample_text[:200] + \"...\")\n",
    "\n",
    "# Tokenisasi kalimat\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(f\"\\nJumlah kalimat: {len(sentences)}\")\n",
    "for i, sentence in enumerate(sentences[:3], 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "# TODO: Implementasikan tokenisasi kata untuk kalimat pertama\n",
    "words = # [LENGKAPI KODE INI]\n",
    "print(f\"\\nJumlah kata: {len(words)}\")\n",
    "print(\"10 kata pertama:\", words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c88f3",
   "metadata": {},
   "source": [
    "## 2. Case Folding dan Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # TODO: Case folding\n",
    "    text = # [LENGKAPI KODE INI]\n",
    "\n",
    "    # TODO: Hilangkan tanda baca menggunakan regex\n",
    "    text = # [LENGKAPI KODE INI]\n",
    "\n",
    "    # TODO: Hilangkan spasi berlebih\n",
    "    text = # [LENGKAPI KODE INI]\n",
    "\n",
    "    return text\n",
    "\n",
    "# Testing\n",
    "test_text = \"Sistem Informasi (SI) adalah TEKNOLOGI!\"\n",
    "normalized = normalize_text(test_text)\n",
    "print(\"Before:\", test_text)\n",
    "print(\"After:\", normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e9275",
   "metadata": {},
   "source": [
    "## 3. Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46aadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "indonesian_stopwords = set(stopwords.words('indonesian'))\n",
    "print(f\"Jumlah stopwords: {len(indonesian_stopwords)}\")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # TODO: Filter token yang bukan stopwords dan panjang > 2\n",
    "    filtered_tokens = # [LENGKAPI KODE INI]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Testing\n",
    "sample_tokens = word_tokenize(normalize_text(\"sistem informasi adalah teknologi untuk manajemen data\"))\n",
    "print(\"Before:\", sample_tokens)\n",
    "filtered = remove_stopwords(sample_tokens)\n",
    "print(\"After:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf7085",
   "metadata": {},
   "source": [
    "## 4. Stemming dengan Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf459a51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Testing stemming\n",
    "test_words = ['penelusuran', 'menggunakan', 'teknologi', 'informasi']\n",
    "print(\"Hasil stemming:\")\n",
    "for word in test_words:\n",
    "    # TODO: Implementasikan stemming\n",
    "    stemmed = # [LENGKAPI KODE INI]\n",
    "    print(f\"{word} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d82409d",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Lengkap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # TODO: Normalisasi teks\n",
    "    text = # [LENGKAPI KODE INI]\n",
    "\n",
    "    # TODO: Tokenisasi\n",
    "    tokens = # [LENGKAPI KODE INI]\n",
    "\n",
    "    # TODO: Filter token yang valid (huruf saja dan panjang > 2)\n",
    "    tokens = # [LENGKAPI KODE INI]\n",
    "\n",
    "    # TODO: Stopword removal\n",
    "    tokens = # [LENGKAPI KODE INI]\n",
    "\n",
    "    # TODO: Stemming\n",
    "    tokens = # [LENGKAPI KODE INI]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Testing preprocessing lengkap\n",
    "for doc in documents[:2]:\n",
    "    processed = preprocess_text(doc['content'])\n",
    "    print(f\"Dokumen {doc['id']}: {doc['title']}\")\n",
    "    print(f\"Token hasil preprocessing: {processed[:10]}...\")\n",
    "    print(f\"Jumlah token: {len(processed)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99e5b5",
   "metadata": {},
   "source": [
    "## Expected Output:\n",
    "Dokumen 1: Sistem informasi\n",
    "\n",
    "Token hasil preprocessing: ['sistem', 'informasi', 'sistem', 'formal', 'sosioteknis', 'organisasional', 'rancang', 'kumpul', 'proses', 'simpan']...\n",
    "\n",
    "Jumlah token: 66\n",
    "\n",
    "Dokumen 2: Pangkalan data\n",
    "\n",
    "Token hasil preprocessing: ['pangkal', 'data', 'basis', 'data', 'bahasa', 'inggris', 'database', 'kumpul', 'data', 'organisir']...\n",
    "\n",
    "Jumlah token: 72"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
